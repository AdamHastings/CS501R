{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "WARNING:tensorflow:From /home/adam/applications/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "ITER\t\tLOSS\t\t\t\tSTYLE_LOSS\t\t\t\tCONTENT LOSS\n",
      "0\t\t2.10544e+11\t\t2.10544e+08\t\t0.0\n",
      "1\t\t2.06324e+11\t\t2.0632e+08\t\t3.79186e+06\n",
      "2\t\t2.02186e+11\t\t2.02171e+08\t\t1.54348e+07\n",
      "3\t\t1.98148e+11\t\t1.98113e+08\t\t3.49036e+07\n",
      "4\t\t1.94219e+11\t\t1.94156e+08\t\t6.21599e+07\n",
      "5\t\t1.90399e+11\t\t1.90302e+08\t\t9.73078e+07\n",
      "6\t\t1.86709e+11\t\t1.86569e+08\t\t1.40117e+08\n",
      "7\t\t1.83149e+11\t\t1.82959e+08\t\t1.90064e+08\n",
      "8\t\t1.79716e+11\t\t1.7947e+08\t\t2.4606e+08\n",
      "9\t\t1.76409e+11\t\t1.76102e+08\t\t3.06926e+08\n",
      "10\t\t1.73222e+11\t\t1.72851e+08\t\t3.71967e+08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import vgg16\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "opt_img = tf.Variable( tf.truncated_normal( [1,224,224,3],\n",
    "                                        dtype=tf.float32,\n",
    "                                        stddev=1e-1), name='opt_img' )\n",
    "\n",
    "tmp_img = tf.clip_by_value( opt_img, 0.0, 255.0 )\n",
    "\n",
    "vgg = vgg16.vgg16( tmp_img, 'vgg16_weights.npz', sess )\n",
    "\n",
    "style_img = imread( 'style.png', mode='RGB' )\n",
    "style_img = imresize( style_img, (224, 224) )\n",
    "style_img = np.reshape( style_img, [1,224,224,3] )\n",
    "\n",
    "content_img = imread( 'content.png', mode='RGB' )\n",
    "content_img = imresize( content_img, (224, 224) )\n",
    "content_img = np.reshape( content_img, [1,224,224,3] )\n",
    "\n",
    "layers = [ 'conv1_1', 'conv1_2',\n",
    "           'conv2_1', 'conv2_2',\n",
    "           'conv3_1', 'conv3_2', 'conv3_3',\n",
    "           'conv4_1', 'conv4_2', 'conv4_3',\n",
    "           'conv5_1', 'conv5_2', 'conv5_3' ]\n",
    "\n",
    "ops = [ getattr( vgg, x ) for x in layers ]\n",
    "\n",
    "content_activation = vgg.conv4_2\n",
    "style_activations = [vgg.conv1_1, vgg.conv2_1, vgg.conv3_1, vgg.conv4_1, vgg.conv5_1]\n",
    "\n",
    "# styles = [getattr(vgg, x) for x in style_activations]\n",
    "# content = getattr(vgg, content_activation)\n",
    "\n",
    "content_acts = sess.run( content_activation, feed_dict={vgg.imgs: content_img } )  # target\n",
    "style_acts = sess.run( style_activations, feed_dict={vgg.imgs: style_img} ) # target\n",
    "\n",
    "target_content = tf.convert_to_tensor(content_acts)\n",
    "target_styles = [tf.convert_to_tensor(l) for l in style_acts]\n",
    "\n",
    "\n",
    "\n",
    "# ops is list of activation nodes for each layer\n",
    "#\n",
    "# --- construct your cost function here\n",
    "#\n",
    "\n",
    "### Content Loss ###\n",
    "# F = tf.convert_to_tensor(content_acts)\n",
    "# P = content_activation\n",
    "# Cast numpy array back into a tensor\n",
    "# tf. constant , initialize with numpy array.\n",
    "# if constant doesn't work, use tf.convert to tensor\n",
    "\n",
    "\n",
    "content_loss_sub = tf.subtract(target_content, content_activation)\n",
    "content_loss_square = tf.square(content_loss_sub)\n",
    "content_loss = 0.5 * tf.reduce_sum(content_loss_square)\n",
    "\n",
    "def computeGram(v):\n",
    "#     height = v.get_shape().as_list()[1]\n",
    "#     width = v.get_shape().as_list()[2]\n",
    "#     channels = v.get_shape().as_list()[3]\n",
    "    _, height, width, channels = v.get_shape().as_list()\n",
    "    v = tf.reshape(v, [height*width, channels])\n",
    "    return tf.matmul(tf.transpose(v), v)\n",
    "\n",
    "### Style Loss ###\n",
    "style_G = [computeGram(l) for l in style_activations]\n",
    "target_style_G = [computeGram(l) for l in target_styles]\n",
    "\n",
    "style_loss_sub = [tf.subtract(i, j) for i, j in zip(target_style_G, style_G)]\n",
    "style_loss_square = [tf.square(a) for a in style_loss_sub]\n",
    "style_loss_rsum = [tf.reduce_sum(a) for a in style_loss_square]\n",
    "M = [style_activations[i].get_shape().as_list()[1] * style_activations[i].get_shape().as_list()[2] for i,l in enumerate(style_activations)]\n",
    "N = [style_activations[i].get_shape().as_list()[3] for i,l in enumerate(style_activations)]\n",
    "style_loss_list = [rs / (4.0 * (m * n)**2) for m, n, rs in zip(M, N, style_loss_rsum)]\n",
    "\n",
    "style_loss = sum(style_loss_list) / 5.0\n",
    "\n",
    "# Relevant snippets from the paper:\n",
    "#   For the images shown in Fig 2 we matched the content representation on layer 'conv4_2'\n",
    "#   and the style representations on layers 'conv1_1', 'conv2_1', 'conv3_1', 'conv4_1' and 'conv5_1'\n",
    "#   The ratio alpha/beta was  1x10-3\n",
    "#   The factor w_l was always equal to one divided by the number of active layers (ie, 1/5)\n",
    "alpha = 1\n",
    "beta = 1000\n",
    "\n",
    "Loss = (alpha * content_loss) + (beta * style_loss)\n",
    "# Loss = content_loss\n",
    "\n",
    "# --- place your adam optimizer call here\n",
    "#     (don't forget to optimize only the opt_img variable)\n",
    "train_step = tf.train.AdamOptimizer(0.1).minimize(Loss, var_list=[opt_img])\n",
    "\n",
    "\n",
    "# this clobbers all VGG variables, but we need it to initialize the\n",
    "# adam stuff, so we reload all of the weights...\n",
    "sess.run( tf.initialize_all_variables() )\n",
    "vgg.load_weights( 'vgg16_weights.npz', sess )\n",
    "\n",
    "# initialize with the content image\n",
    "sess.run( opt_img.assign( content_img ))\n",
    "\n",
    "# --- place your optimization loop here\n",
    "\n",
    "EPOCHS = 10\n",
    "# No placeholders in this lab\n",
    "print \"ITER\\t\\tLOSS\\t\\t\\tSTYLE_LOSS\\t\\t\\tCONTENT LOSS\"\n",
    "\n",
    "for i in range(EPOCHS+1):\n",
    "    loss, c_loss, s_loss, _ = sess.run([Loss, content_loss, style_loss, train_step])\n",
    "    \n",
    "    if ((i%50) == 0):\n",
    "        # save tmp_img\n",
    "        # use imsave\n",
    "        fname = epoch_ + str(i)\n",
    "        imsave(fname, tmp_img)\n",
    "        print str(i) + \"\\t\\t\" + str(loss) + \"\\t\\t\" + str(s_loss) + \"\\t\\t\" + str(c_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
