{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from textloader import TextLoader\n",
    "from tensorflow.python.ops.rnn_cell import BasicLSTMCell, MultiRNNCell, RNNCell\n",
    "from tensorflow.contrib.legacy_seq2seq import sequence_loss, rnn_decoder\n",
    "from mygru import mygru\n",
    "\n",
    "\n",
    "#\n",
    "# -------------------------------------------\n",
    "#\n",
    "# Global variables\n",
    "\n",
    "batch_size = 50 # 50 \n",
    "sequence_length = 50 # 50\n",
    "\n",
    "data_loader = TextLoader( \".\", batch_size, sequence_length )\n",
    "\n",
    "vocab_size = data_loader.vocab_size  # dimension of one-hot encodings\n",
    "state_dim = 128\n",
    "\n",
    "# num_layers = 2 # This is no longer being used\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "#\n",
    "\n",
    "# define placeholders for our inputs.  \n",
    "# in_ph is assumed to be [batch_size,sequence_length]\n",
    "# targ_ph is assumed to be [batch_size,sequence_length]\n",
    "\n",
    "in_ph = tf.placeholder( tf.int32, [ batch_size, sequence_length ], name='my_inputs' )\n",
    "targ_ph = tf.placeholder( tf.int32, [ batch_size, sequence_length ], name='targets' )\n",
    "in_onehot = tf.one_hot( in_ph, vocab_size, name=\"input_onehot\" )\n",
    "\n",
    "inputs = tf.split( in_onehot, sequence_length, axis=1 )\n",
    "inputs = [ tf.squeeze(input_, [1]) for input_ in inputs ]\n",
    "targets = tf.split( targ_ph, sequence_length, axis=1 )\n",
    "\n",
    "# at this point, inputs is a list of length sequence_length\n",
    "# each element of inputs is [batch_size,vocab_size]\n",
    "\n",
    "# targets is a list of length sequence_length\n",
    "# each element of targets is a 1D vector of length batch_size\n",
    "\n",
    "# ------------------\n",
    "# YOUR COMPUTATION GRAPH HERE\n",
    "\n",
    "# create a BasicLSTMCell\n",
    "gru0 = mygru(state_dim);\n",
    "gru1 = mygru(state_dim);\n",
    "\n",
    "# use it to create a MultiRNNCell\n",
    "my_rnn = MultiRNNCell([gru0, gru1], state_is_tuple=True)\n",
    "\n",
    "# use it to create an initial_state\n",
    "# note that initial_state will be a *list* of tensors!\n",
    "initial_state = my_rnn.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# call seq2seq.rnn_decoder\n",
    "with tf.variable_scope(\"encoder\") as scope:\n",
    "    outputs, final_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, my_rnn)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([state_dim, vocab_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_normal([vocab_size], stddev=0.01))\n",
    "\n",
    "# transform the list of state outputs to a list of logits.\n",
    "# use a linear transformation.\n",
    "logits = [tf.matmul(output, W) +[b] * batch_size for output in outputs]\n",
    "\n",
    "# call seq2seq.sequence_loss\n",
    "loss_w = [1.0 for i in range(sequence_length)]\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(logits, targets, loss_w)\n",
    "\n",
    "# create a training op using the Adam optimizer\n",
    "optim = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "# ------------------\n",
    "# YOUR SAMPLER GRAPH HERE\n",
    "\n",
    "# Reuse of variables just means you're supposed to reuse the RNN variable\n",
    "\n",
    "# place your sampler graph here it will look a lot like your\n",
    "# computation graph, except with a \"batch_size\" of 1.\n",
    "\n",
    "# remember, we want to reuse the parameters of the cell and whatever\n",
    "# parameters you used to transform state outputs to logits!\n",
    "\n",
    "\n",
    "\n",
    "s_batch_size = 1\n",
    "s_sequence_length = 1\n",
    "\n",
    "s_in_ph = tf.placeholder( tf.int32, [ s_batch_size ], name='s_inputs' )\n",
    "s_in_onehot = [tf.one_hot( s_in_ph, vocab_size, name=\"s_input_onehot\" )]\n",
    "\n",
    "\n",
    "# use it to create an initial_state\n",
    "# note that initial_state will be a *list* of tensors!\n",
    "s_initial_state = my_rnn.zero_state(s_batch_size, tf.float32)\n",
    "\n",
    "# call seq2seq.rnn_decoder\n",
    "with tf.variable_scope(\"decoder\") as scope:\n",
    "    s_outputs, s_final_state = tf.contrib.legacy_seq2seq.rnn_decoder(s_in_onehot, s_initial_state, my_rnn)\n",
    "\n",
    "# transform the list of state outputs to a list of logits.\n",
    "# use a linear transformation.\n",
    "# s_logits = [tf.matmul(s_output, W) +[b] * s_batch_size for s_output in s_outputs]\n",
    "# s_probs = tf.nn.softmax(s_logits);\n",
    "s_probs = tf.matmul(tf.cast(s_outputs[0], tf.float32), W) + b\n",
    "\n",
    "#\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "#\n",
    "\n",
    "def sample( num=200, prime='ab' ):\n",
    "\n",
    "    # prime the pump \n",
    "\n",
    "    # generate an initial state. this will be a list of states, one for\n",
    "    # each layer in the multicell.\n",
    "    s_state = sess.run( s_initial_state )\n",
    "\n",
    "    # for each character, feed it into the sampler graph and\n",
    "    # update the state.\n",
    "    for char in prime[:-1]:\n",
    "        x = np.ravel( data_loader.vocab[char] ).astype('int32')\n",
    "        feed = { s_in_ph:x }\n",
    "        for i, s in enumerate( s_initial_state ):\n",
    "            feed[s] = s_state[i]\n",
    "        s_state = sess.run( s_final_state, feed_dict=feed )\n",
    "\n",
    "    # now we have a primed state vector; we need to start sampling.\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.ravel( data_loader.vocab[char] ).astype('int32')\n",
    "\n",
    "        # plug the most recent character in...\n",
    "        feed = { s_in_ph:x }\n",
    "        for i, s in enumerate( s_initial_state ):\n",
    "            feed[s] = s_state[i]\n",
    "        ops = [s_probs]\n",
    "        ops.extend( list(s_final_state) )\n",
    "\n",
    "        retval = sess.run( ops, feed_dict=feed )\n",
    "\n",
    "        s_probsv = retval[0]\n",
    "        s_state = retval[1:]\n",
    "\n",
    "        # ...and get a vector of probabilities out!\n",
    "\n",
    "        # now sample (or pick the argmax)\n",
    "        sample = np.argmax( s_probsv[0] )\n",
    "        # sample = np.random.choice( vocab_size, p=s_probsv[0] )\n",
    "\n",
    "        pred = data_loader.chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "\n",
    "    return ret\n",
    "\n",
    "#\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "#\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run( tf.global_variables_initializer() )\n",
    "summary_writer = tf.summary.FileWriter( \"./tf_logs\", graph=sess.graph )\n",
    "\n",
    "lts = []\n",
    "\n",
    "print \"FOUND %d BATCHES\" % data_loader.num_batches\n",
    "\n",
    "for j in range(1000):\n",
    "\n",
    "    state = sess.run( initial_state )\n",
    "    data_loader.reset_batch_pointer()\n",
    "\n",
    "    for i in range( data_loader.num_batches ):\n",
    "        \n",
    "        x,y = data_loader.next_batch()\n",
    "\n",
    "        # we have to feed in the individual states of the MultiRNN cell\n",
    "        feed = { in_ph: x, targ_ph: y }\n",
    "        for k, s in enumerate( initial_state ):\n",
    "            feed[s] = state[k]\n",
    "\n",
    "        ops = [optim,loss]\n",
    "        ops.extend( list(final_state) )\n",
    "\n",
    "        # retval will have at least 3 entries:\n",
    "        # 0 is None (triggered by the optim op)\n",
    "        # 1 is the loss\n",
    "        # 2+ are the new final states of the MultiRNN cell\n",
    "        retval = sess.run( ops, feed_dict=feed )\n",
    "\n",
    "        lt = retval[1]\n",
    "        state = retval[2:]\n",
    "\n",
    "        if i%1000==0:\n",
    "            print \"%d %d\\t%.4f\" % ( j, i, lt )\n",
    "            lts.append( lt )\n",
    "\n",
    "    print sample( num=60, prime=\"And \" )\n",
    "    # print sample( num=60, prime=\"ababab\" )\n",
    "    # print sample( num=60, prime=\"foo ba\" )\n",
    "    # print sample( num=60, prime=\"abcdab\" )\n",
    "\n",
    "summary_writer.close()\n",
    "\n",
    "#\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "#\n",
    "\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.plot( lts )\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mygru Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell import RNNCell\n",
    "from tensorflow.python.ops.rnn_cell_impl import _linear\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "\n",
    "class mygru( RNNCell ):\n",
    " \n",
    "    def __init__( self, num_units):\n",
    "        self._num_units = num_units\n",
    " \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    " \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    " \n",
    "    def __call__( self, inputs, state, scope=None ):\n",
    "        \n",
    "        sigmoid = math_ops.sigmoid\n",
    "        tanh = math_ops.tanh\n",
    "\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            with tf.variable_scope(\"r\"):\n",
    "                r = sigmoid(_linear([inputs, state], self._num_units, True))\n",
    "            with tf.variable_scope(\"z\"):\n",
    "                z = sigmoid(_linear([inputs, state], self._num_units, True))\n",
    "            with tf.variable_scope(\"h_tilde\"):\n",
    "                h_tilde = tanh(_linear([inputs, r * state], self._num_units, True))\n",
    "            new_h = (z * state) + ((1 - z) * h_tilde)\n",
    "        \n",
    "        return new_h, new_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book of Alma Samples\n",
    "\n",
    "1. And the pand the partore the pand the reond the reond the reond \n",
    "2. And the propent of the Lamanites of the Lamanites of the Lamanit\n",
    "5. And the people of the Lamanites were and the land of Nephihah ha\n",
    "7. And the people of Nephihah, and they were and the land of Nephi,\n",
    "8. And the people of Nephi, and the Lamanites were and the land of\n",
    "9. And the land of Moroni and the land of Nephi, that they might be\n",
    "11. And the land of Moroni and the land of Moroni, and the land of M\n",
    "12. And inting the people the people the people of the Lamanites of\n",
    "13. And intiling the people of the Lamanites which had sont the peop\n",
    "14. And it came to pass that the Lamanites which had sont the people\n",
    "15. And it came to pass that the Lamanites which had said unto the L\n",
    "16. And it came to pass that the Lamanites which have been suppose t\n",
    "17. And it came to pass that they were strong the people of the Lama\n",
    "18. And it came to pass that they were all the people of the Lamanit\n",
    "19. And it came to pass that they were all the people of Nephi, and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walt Whitman Samples\n",
    "\n",
    "3. I and and the sore and the sore and the sore and the sore and\n",
    "5. I stars of the stall of the stall of the shead and the sall of\n",
    "6. I am the stares of the starn and the stares, \n",
    "7. I am the shouse and the starn and the shore of the shore of th\n",
    "8. The surrounder and the shall of the shall of t\n",
    "9. I am the part of the sumpers, \n",
    " The should of the childern and \n",
    "10. I am the pooter’d with his ampled, \n",
    " And the shore and the shor\n",
    "11. I am the cond of the brood, \n",
    " The driver the sturn and the dusk\n",
    "12. I am long, \n",
    " The dest of the stars of the sun-room, and the dis\n",
    "13. I ancher’d with the sun, \n",
    " Where the handshis and the lingle st\n",
    "14. I lift to ston and fle myself, \n",
    " And the head hand of the broth\n",
    "15. I and works of the trees of my own from the sea, \n",
    " Where the ha\n",
    "16. I life and forelight, \n",
    " Soundsing in a reard and flower it is a\n",
    "17. I life and sun, \n",
    " And the perfect of plains the sea! I resist t\n",
    "18. I lift and rest the steam witter’d, and never will be all a li\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
